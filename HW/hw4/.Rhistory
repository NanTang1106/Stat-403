}
fit_coeff
boxplot(fit_coeff, col="palegreen")
fit_err = t(t(fit_coeff)-beta1)
boxplot(fit_err, ylim=c(-1.0,1.0), col="palegreen",
main=paste("n =",n), ylab="Error of each coefficient")
abline(h=0, lwd=3, col="purple")
## animation
while(tmp>0){
for(n in c(25,50,100,200,500,1000, 2000)){
fit_coeff = matrix(NA, nrow=N, ncol=6)
colnames(fit_coeff) = c("Intercept","X1", "X2","X3","X4","X5")
for(i in 1:N){
X = matrix(rnorm(n*5),ncol=5)
Y = beta1[1] + X %*% beta1[2:6] + rnorm(n,sd=2)
fit = lm(Y~X)
fit_coeff[i,] = fit$coefficients
}
fit_err = t(t(fit_coeff)-beta1)
boxplot(fit_err, ylim=c(-1.0,1.0), col="palegreen",
main=paste("n =",n), ylab="Error of each coefficient")
abline(h=0, lwd=3, col="purple")
Sys.sleep(1)
}
}
###                                             ###
### Part 2: Linear Regression - Model Selection ###
###                                             ###
### Errors in regression
n = 200
d = 20
X = matrix(rnorm(n*d), nrow=n)
Y = 1*X[,1]+2*X[,2]+3*X[,3] + rnorm(n,sd=0.5)
true_coeff = c(0,1,2,3,rep(0,17))
fit = lm(Y~X)
summary(fit)
sum(summary(fit)$residuals^2)
err_coeff = sum((summary(fit)$coeff[,1]-true_coeff)^2)
err_coeff
X = matrix(rnorm(n*d), nrow=n)
Y = 1*X[,1]+2*X[,2]+3*X[,3] + rnorm(n,sd=0.5)
true_coeff = c(0,1,2,3,rep(0,17))
rnorm(10*20)
Y = 1*X[,1]+2*X[,2]+3*X[,3] + rnorm(n,sd=0.5)
true_coeff = c(0,1,2,3,rep(0,17))
X = matrix(rnorm(n*d), nrow=n)
X
?matrix
fit = lm(Y~X)
summary(fit)
sum(summary(fit)$residuals^2)
err_coeff = sum((summary(fit)$coeff[,1]-true_coeff)^2)
err_coeff
info_fit = matrix(NA, nrow=20, ncol=2)
colnames(info_fit) = c("Sum of Sqrt Res.", "Error in Coeff")
for(i in 1:20){
fit = lm(Y~X[,1:i])
info_fit[i,1] = sum(summary(fit)$residuals^2)
fit_coeff = rep(0, 21)
fit_coeff[1:(i+1)] = summary(fit)$coeff[,1]
err_coeff = sum((fit_coeff-true_coeff)^2)
info_fit[i,2] = err_coeff
}
info_fit
plot(x=1:20, y=info_fit[,1], type="l", lwd=2, ylab="Sum of Sqrt Res.",
xlab="# of covariates")
plot(x=1:20, y=info_fit[,1], type="l", lwd=2, ylab="Sum of Sqrt Res.",
xlab="# of covariates", ylim=c(44,48))
plot(x=1:20, y=info_fit[,1], type="l", lwd=2, ylab="Sum of Sqrt Res.",
xlab="# of covariates", ylim=c(44,48))
plot(x=1:20, y=info_fit[,2], type="l", lwd=2,
ylab="Sum of MSE of all variables",
xlab="# of covariates")
### model selection
dat = data.frame(Y,X)
View(dat)
fit_full = lm(Y~.,data=dat)
fit_null = lm(Y~1,data=dat)
fir_full
fit_full
fit_null
## AIC
F_fit = step(fit_null, scope=list(lower=fit_null,upper=fit_full),
direction="forward")
B_fit = step(fit_full, scope=list(lower=fit_null,upper=fit_full),
direction="backward")
T_fit = step(fit_full, scope=list(lower=fit_null,upper=fit_full),
direction="both")
## BIC
F_fit_bic = step(fit_null, scope=list(lower=fit_null,upper=fit_full),
direction="forward", k=log(nrow(dat)))
B_fit_bic = step(fit_full, scope=list(lower=fit_null,upper=fit_full),
direction="backward", k=log(nrow(dat)))
T_fit = step(fit_full, scope=list(lower=fit_null,upper=fit_full),
direction="both", k=log(nrow(dat)))
### apply to a real data
data0 = read.table("abalone.data", sep=",")
head(data0)
names(data0) = c("Sex","Length","Diameter","Height","Whole weight",
"Shucked weight", "Viscera weight","Shell weight","Rings")
head(data0)
fit_full = lm(Rings~., data=data0)
fit_null = lm(Rings~1, data=data0)
## AIC
F_fit = step(fit_null, scope=list(lower=fit_null,upper=fit_full),
direction="forward")
B_fit = step(fit_full, scope=list(lower=fit_null,upper=fit_full),
direction="backward")
## BIC
B_fit_bic = step(fit_full, scope=list(lower=fit_null,upper=fit_full),
direction="backward", k=log(nrow(data0)))
F_fit_bic = step(fit_null, scope=list(lower=fit_null,upper=fit_full),
direction="forward", k=log(nrow(data0)))
## both
step(fit_full, scope=list(lower=fit_null),
direction="both")
step(fit_full, scope=list(lower=fit_null),
direction="both", k=log(nrow(data0)))
###                              ###
### Part 3: Logistic Regression  ###
###                              ###
D = read.csv("binary.csv")
head(D)
summary(D)
xtabs(~admit + rank, data = D)
getwd()
setwd('/Users/nantang/Google Drive/STAT 403/HW/HW4')
quakes <- read.table('fijiquakes.dat', sep='')
quakes
quakes <- read.table('fijiquakes.dat', sep='', header=T)
head(quakes)
###                              ###
### Part 3: Logistic Regression  ###
###                              ###
D = read.csv("binary.csv")
head(D)
summary(D)
xtabs(~admit + rank, data = D)
?glm
D_logistic = glm(admit ~ gre + gpa + rank,
data = D, family = "binomial")
D_logistic
summary(D)
summary(D_logistic)
confint.default(D_logistic, level = 0.9)
## treat each rank as different categories
D$rank = factor(D$rank)
D_logistic_r = glm(admit ~ gre + gpa + rank,
data = D, family = "binomial")
summary(D_logistic_r)
confint.default(D_logistic_r, level = 0.9)
predict(D_logistic_r, type="response")
## making 90% CI for different values of gre
alpha_CI = 0.95
multiplier = qnorm(alpha_CI)
D_gre1 = data.frame(gre=seq(from=200,to=800, by=4),
gpa=mean(D$gpa), rank=factor(1))
D_gre_fit = predict(D_logistic_r, type="response", newdata=D_gre1,
se=T)
D_gre1$p = D_gre_fit$fit
D_gre1$p_U = D_gre_fit$fit+multiplier*D_gre_fit$se.fit
D_gre1$p_L = D_gre_fit$fit-multiplier*D_gre_fit$se.fit
head(D_gre1)
## plot
plot(x=D_gre1$gre, y=D_gre1$p, ylim=c(0,0.7), type="l", lwd=6,
col="blue", xlab="GRE", ylab="Probability of Getting Admission")
lines(x=D_gre1$gre, y=D_gre1$p_U, col="dodgerblue", lwd=3,
lty=2)
lines(x=D_gre1$gre, y=D_gre1$p_L, col="dodgerblue", lwd=3,
lty=2)
## CF: rank-2 school
D_gre2 = data.frame(gre=seq(from=200,to=800, by=4),
gpa=mean(D$gpa), rank=factor(2))
D_gre_fit = predict(D_logistic_r, type="response", newdata=D_gre2,
se=T)
D_gre2$p = D_gre_fit$fit
D_gre2$p_U = D_gre_fit$fit+multiplier*D_gre_fit$se.fit
D_gre2$p_L = D_gre_fit$fit-multiplier*D_gre_fit$se.fit
head(D_gre2)
plot(x=D_gre1$gre, y=D_gre1$p, ylim=c(0,0.7), type="l", lwd=6,
col="blue", xlab="GRE", ylab="Probability of Getting Admission")
lines(x=D_gre1$gre, y=D_gre1$p_U, col="dodgerblue", lwd=3,lty=2)
lines(x=D_gre1$gre, y=D_gre1$p_L, col="dodgerblue", lwd=3,lty=2)
lines(x=D_gre2$gre, y=D_gre2$p, lwd=6, col="limegreen")
lines(x=D_gre2$gre, y=D_gre2$p_U, col="palegreen", lwd=3,lty=2)
lines(x=D_gre2$gre, y=D_gre2$p_L, col="palegreen", lwd=3,lty=2)
#### Exercise 3:
#### We will use logistic regression to analyze the 'attitude' data again.
#### But now we define a new variable "class" such that those observations
#### (clerks) wit 'rating' > 65 belong to "class 1" and the others are in
#### "class 0". Namely,
Y = attitude$rating>65
bern_p <- function(x) {
return(exp(1 + 2 * x) / (1 + exp(1 + 2 * x)))
}
bern_p(1)
bern_p(0)
exp(1)/(1+exp(1))
x_unif <- runif(n)
?runif
?bernou
y_value <- rbern(n, bern_p(x_value))
?rbinom
y_value <- rbinom(n, size=1, p=bern_p(x_value))
x_value <- runif(n)
y_value <- rbinom(n, size=1, p=bern_p(x_value))
n <- 500
x_value <- runif(n)
y_value <- rbinom(n, size=1, p=bern_p(x_value))
head(y_value)
y_value
sum(y_value)/n
xy_logic = glm(y_value~x_value, family = "binomial")
summary(xy_logic)
?lm
xtabs(x_value, y_value)
summary(xy_logic)[]1,1
summary(xy_logic)[1,1]
summary(xy_logic)$coefficient
beta0 <- summary(xy_logic)$coefficient[1,1]
beta1 <- summary(xy_logic)$coefficient[2,1]
beta0
beta1
n <- 500
x_value <- runif(n)
y_value <- rbinom(n, size=1, p=bern_p(x_value))
xy_logic = glm(y_value~x_value, family = "binomial")
beta0 <- summary(xy_logic)$coefficient[1,1]
beta1 <- summary(xy_logic)$coefficient[2,1]
beta0
beta1
n <- 500
x_value <- runif(n)
y_value <- rbinom(n, size=1, p=bern_p(x_value))
xy_logic = glm(y_value~x_value, family = "binomial")
beta0 <- summary(xy_logic)$coefficient[1,1]
beta1 <- summary(xy_logic)$coefficient[2,1]
beta0
beta1
n <- 500
x_value <- runif(n)
y_value <- rbinom(n, size=1, p=bern_p(x_value))
xy_logic = glm(y_value~x_value, family = "binomial")
beta0 <- summary(xy_logic)$coefficient[1,1]
beta1 <- summary(xy_logic)$coefficient[2,1]
beta0
beta1
n <- 500
x_value <- runif(n)
y_value <- rbinom(n, size=1, p=bern_p(x_value))
xy_logic = glm(y_value~x_value, family = "binomial")
beta0 <- summary(xy_logic)$coefficient[1,1]
beta1 <- summary(xy_logic)$coefficient[2,1]
beta0
beta1
list(1:10)
list(rep(c(NA, NA), 10))
N = 2000
beta0_sim <- rep(NA, N)
beta1_sim <- rep(NA, N)
for (ii in 1:N) {
x_value <- runif(n)
y_value <- rbinom(n, size=1, p=bern_p(x_value))
xy_logic = glm(y_value~x_value, family = "binomial")
beta0_sim[ii] <- summary(xy_logic)$coefficient[1,1]
beta1_sim[ii] <- summary(xy_logic)$coefficient[2,1]
}
mean(beta0_sim)
mean(beta1_sim)
head(beta0_sim)
hist(beta0_sim)
hist(beta0_sim, breaks=20)
hist(beta0_sim, breaks=30)
hist(beta0_sim, probability=T, main='Histogram of Fitted Beta0',
xlab='Fitted Beta0', col=rgb(0,0,1))
hist(beta0_sim, probability=T, main='Histogram of Fitted Beta0',
xlab='Fitted Beta0', col=rgb(0,0,1,0.8))
hist(beta0_sim, probability=T, main='Histogram of Fitted Beta0',
xlab='Fitted Beta0', col=rgb(0,0,0.9,0.8))
hist(beta0_sim, probability=T, main='Histogram of Fitted Beta0',
xlab='Fitted Beta0', col=rgb(0,0,0.9,1))
hist(beta0_sim, probability=T, main='Histogram of Fitted Beta0',
xlab='Fitted Beta0', col=rgb(0,1,0.9,1))
hist(beta0_sim, probability=T, main='Histogram of Fitted Beta0',
xlab='Fitted Beta0', col=rgb(0,0.5,0.5,1))
hist(beta0_sim, probability=T, main='Histogram of Fitted Beta0',
xlab='Fitted Beta0', col=rgb(0,0.5,0,1))
hist(beta0_sim, probability=T, main='Histogram of Fitted Beta0',
xlab='Fitted Beta0', col=rgb(1,0.5,0,1))
hist(beta0_sim, probability=T, main='Histogram of Fitted Beta0',
xlab='Fitted Beta0', col=rgb(1,1,0,1))
hist(beta0_sim, probability=T, main='Histogram of Fitted Beta0',
xlab='Fitted Beta0', col=rgb(1,1,0,0.6))
hist(beta0_sim, probability=T, main='Histogram of Fitted Beta0',
xlab='Fitted Beta0', col=rgb(1,1,0,0.75))
hist(beta0_sim, probability=T, main='Histogram of Fitted Beta0',
xlab='Fitted Beta0', col=rgb(0,1,1,0.75))
hist(beta0_sim, probability=T, main='Histogram of Fitted Beta0',
xlab='Fitted Beta0', col=rgb(0,0.5,1,0.75))
hist(beta0_sim, probability=T, main='Histogram of Fitted Beta0',
xlab='Fitted Beta0', col=rgb(0,0.6,1,0.75))
hist(beta0_sim, probability=T, main='Histogram of Fitted Beta0',
xlab='Fitted Beta0', col=rgb(0,0.8,1,0.75))
hist(beta0_sim, probability=T, main='Histogram of Fitted Beta0',
xlab='Fitted Beta0', col=rgb(0,0.8,0.9,0.75))
hist(beta0_sim, probability=T, main='Histogram of Fitted Beta0',
xlab='Fitted Beta0', col=rgb(0,0.9,0.9,0.75))
abline(x=1)
?abline
abline(v=1)
hist(beta0_sim, probability=T, main='Histogram of Fitted Beta0',
xlab='Fitted Beta0', col='coral1')
abline(v=1, lwd=3, col='chartreuse2' )
abline(v=1, lwd=3, col='cadetblue1' )
abline(v=1, lwd=3, col='cornflowerblue' )
legend('topright', 'true beta0', col='cornflowerblue')
legend('topright', 'true beta0', col='cornflowerblue', lwd=2)
legend('topright', 'true beta0', col='cornflowerblue', lwd=2, cex=0.75)
legend('topright', 'True Beta0', col='cornflowerblue', lwd=3, cex=0.75)
hist(beta0_sim, probability=T, main='Histogram of Fitted Beta0',
xlab='Fitted Beta0', col='coral1')
abline(v=1, lwd=3, col='cornflowerblue' )
legend('topright', 'True Beta0', col='cornflowerblue', lwd=3, cex=0.75)
legend('topright', 'True Beta1', col='coral1', lwd=3, cex=0.75)
hist(beta1_sim, probability=T, main='Histogram of Fitted Beta1',
xlab='Fitted Beta1', col='cornflowerblue')
abline(v=1, lwd=3, col='cornflowerblue' )
legend('topright', 'True Beta1', col='coral1', lwd=3, cex=0.75)
hist(beta1_sim, probability=T, main='Histogram of Fitted Beta1', breaks=20
xlab='Fitted Beta1', col='cornflowerblue')
hist(beta1_sim, probability=T, main='Histogram of Fitted Beta1', breaks=20,
xlab='Fitted Beta1', col='cornflowerblue')
abline(v=2, lwd=3, col='cornflowerblue' )
abline(v=2, lwd=3, col='coral1' )
hist(beta1_sim, probability=T, main='Histogram of Fitted Beta1', breaks=20,
xlab='Fitted Beta1', col='cornflowerblue')
abline(v=2, lwd=3, col='coral1' )
legend('topright', 'True Beta1', col='coral1', lwd=3, cex=0.75)
hist(beta0_sim, probability=T, main='Histogram of Fitted Beta0', breaks=20,
xlab='Fitted Beta0', col='coral1')
abline(v=1, lwd=3, col='cornflowerblue' )
legend('topright', 'True Beta0', col='cornflowerblue', lwd=3, cex=0.75)
n = 2000
N = 2000
beta0_sim <- rep(NA, N)
beta1_sim <- rep(NA, N)
for (ii in 1:N) {
x_value <- runif(n)
y_value <- rbinom(n, size=1, p=bern_p(x_value))
xy_logic = glm(y_value~x_value, family = "binomial")
beta0_sim[ii] <- summary(xy_logic)$coefficient[1,1]
beta1_sim[ii] <- summary(xy_logic)$coefficient[2,1]
}
hist(beta0_sim, probability=T, main='Histogram of Fitted Beta0', breaks=20,
xlab='Fitted Beta0', col='coral1')
abline(v=1, lwd=3, col='cornflowerblue' )
legend('topright', 'True Beta0', col='cornflowerblue', lwd=3, cex=0.75)
hist(beta1_sim, probability=T, main='Histogram of Fitted Beta1', breaks=20,
xlab='Fitted Beta1', col='cornflowerblue')
abline(v=2, lwd=3, col='coral1' )
legend('topright', 'True Beta1', col='coral1', lwd=3, cex=0.75)
hist(beta0_sim, probability=T, main='Histogram of Fitted Beta0', breaks=20,
xlab='Fitted Beta0', col='coral1', xlim=c(0.2, 1.8))
abline(v=1, lwd=3, col='cornflowerblue' )
hist(beta1_sim, probability=T, main='Histogram of Fitted Beta1', breaks=20,
xlab='Fitted Beta1', col='cornflowerblue', xlim=c(0.5, 4.0))
abline(v=2, lwd=3, col='coral1' )
legend('topright', 'True Beta1', col='coral1', lwd=3, cex=0.75)
hist(beta0_sim, probability=T, main='Histogram of Fitted Beta0', breaks=20,
xlab='Fitted Beta0', col='coral1', xlim=c(0.2, 1.8))
abline(v=1, lwd=3, col='cornflowerblue' )
legend('topright', 'True Beta0', col='cornflowerblue', lwd=3, cex=0.75)
MSE(1, beta0_sim)
?mse
?MSE
bias
?bias
?predict
predict(xy_logic)
install.packages("Metrics")
library(Metrics)
?mse
sample_size <- seq(from=100, to=2000, by=100)
length(sample_size)
sample_sizes <- seq(from=100, to=2000, by=100)
beta1_mse <- rep(NA, length(sample_size))
N = 2000
for (ii in 1:length(sample_sizes)) {
n <- sample_sizes[ii]
beta1 <- rep(NA, N)
for (jj in 1:N) {
x_value <- runif(n)
y_value <- rbinom(n, size=1, p=bern_p(x_value))
xy_logic = glm(y_value~x_value, family = "binomial")
beta1[jj] <- summary(xy_logic)$coefficient[2,1]
}
beta1_mse[ii] <- mse(beta1, 2)
}
plot(x=sample_sizes, y=beta1_mse)
beta1_mse
sample_sizes <- seq(from=100, to=2000, by=200)
length(sample_sizes)
beta1_mse <- rep(NA, length(sample_size))
N = 2000
for (ii in 1:length(sample_sizes)) {
n <- sample_sizes[ii]
beta1 <- rep(NA, N)
for (jj in 1:N) {
x_value <- runif(n)
y_value <- rbinom(n, size=1, p=bern_p(x_value))
xy_logic = glm(y_value~x_value, family = "binomial")
beta1[jj] <- summary(xy_logic)$coefficient[2,1]
}
beta1_mse[ii] <- mse(beta1, 2)
}
plot(x=sample_sizes, y=beta1_mse)
beta1_mse
beta1_mse <- rep(NA, length(sample_size))
sample_sizes <- seq(from=100, to=2000, by=200)
beta1_mse <- rep(NA, length(sample_size))
sample_sizes <- seq(from=100, to=2000, by=200)
beta1_mse <- rep(NA, length(sample_sizes))
N = 2000
for (ii in 1:length(sample_sizes)) {
n <- sample_sizes[ii]
beta1 <- rep(NA, N)
for (jj in 1:N) {
x_value <- runif(n)
y_value <- rbinom(n, size=1, p=bern_p(x_value))
xy_logic = glm(y_value~x_value, family = "binomial")
beta1[jj] <- summary(xy_logic)$coefficient[2,1]
}
beta1_mse[ii] <- mse(beta1, 2)
}
plot(x=sample_sizes, y=beta1_mse)
?pch
plot(x=sample_sizes, y=beta1_mse, pch=20, cex=2, col='green')
lines(x=sample_sizes, y=beta1_mse, lwd=3, col="skyblue")
grid(nx=NA,ny=NULL,lty=1,lwd=0.5,col="gray")
plot(x=sample_sizes, y=beta1_mse, pch=20, cex=2, col='green', xlim=c(100, 2000))
lines(x=sample_sizes, y=beta1_mse, lwd=3, col="skyblue")
plot(x=sample_sizes, y=beta1_mse, pch=20, cex=2, col='coral1', xlim=c(100, 2000),
xlab='Sample Size', ylab='MSE of Estimated Beta1',
main='MSE of Estimated Beta1 VS Sample Size')
lines(x=sample_sizes, y=beta1_mse, lwd=3, col="skyblue")
plot(x=sample_sizes, y=beta1_mse, type=ln, col='coral1', xlim=c(100, 2000),
xlab='Sample Size', ylab='MSE of Estimated Beta1',
main='MSE of Estimated Beta1 VS Sample Size')
plot(x=sample_sizes, y=beta1_mse, type='ln', col='coral1', xlim=c(100, 2000),
xlab='Sample Size', ylab='MSE of Estimated Beta1',
main='MSE of Estimated Beta1 VS Sample Size')
plot(x=sample_sizes, y=beta1_mse, type='ln', lwd=3, col='coral1', xlim=c(100, 2000),
xlab='Sample Size', ylab='MSE of Estimated Beta1',
main='MSE of Estimated Beta1 VS Sample Size')
points(x=sample_sizes, y=beta1_mse, pch=20, cex=2, col="skyblue")
grid(nx=NA,ny=NULL,lty=1,lwd=0.5,col="gray")
###                              ###
### Part 4: Rejection Sampling   ###
###                              ###
### We consider sampling points from a distribution f(x)~ sin(x) within
### the interval [0,pi]. In this case, the corresponding density
### f(x) = 0.5*sin(x).
f_target = function(x){
return(0.5*sin(x))
}
### We first try p(x)=Unif[0,pi]. We set M = 2.
M = 2
f_target = function(x){
return(0.5*sin(x))
}
### We first try p(x)=Unif[0,pi]. We set M = 2.
M = 2
N = 50000
Y = runif(N, min=0,max=pi)
U0 = runif(N)
X = Y[which(U0<f_target(Y)/M/(1/pi))]
hist(X, breaks=30, col="skyblue", probability = T)
l_seq = seq(from=0,to=pi,length.out=1001)
lines(x=l_seq, y=f_target(l_seq),lwd=3, col="red")
length(X)/N
Y = runif(N, min=0,max=pi)
## the effect of the constant M
for(M in seq(from=0.5, to=5, by=0.5)){
Y = runif(N, min=0,max=pi)
U0 = runif(N)
X = Y[which(U0<f_target(Y)/M/(1/pi))]
hist(X, breaks=30, col="skyblue", probability = T,
ylim=c(0,0.6),
main=paste("M =",M,"; Acceptance Rate :", length(X)/N))
lines(x=l_seq, y=f_target(l_seq),lwd=3, col="red")
Sys.sleep(1)
}
for(M in seq(from=0.5, to=5, by=0.5)){
Y = runif(N, min=0,max=pi)
U0 = runif(N)
X = Y[which(U0<f_target(Y)/M/(1/pi))]
hist(X, breaks=30, col="skyblue", probability = T,
ylim=c(0,0.6),
main=paste("M =",M,"; Acceptance Rate :", length(X)/N))
lines(x=l_seq, y=f_target(l_seq),lwd=3, col="red")
Sys.sleep(1)
}
for(M in seq(from=0.5, to=5, by=0.5)){
Y = runif(N, min=0,max=pi)
U0 = runif(N)
X = Y[which(U0<f_target(Y)/M/(1/pi))]
hist(X, breaks=30, col="skyblue", probability = T,
ylim=c(0,0.6),
main=paste("M =",M,"; Acceptance Rate :", length(X)/N))
lines(x=l_seq, y=f_target(l_seq),lwd=3, col="red")
Sys.sleep(1)
}
