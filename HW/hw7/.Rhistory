for(i in 1:B){
w = sample(n,n,replace=T)
dat_BT = dat[w]
dat_kde_BT = density(dat_BT, from = 20, to=120, n=1000, bw=h0)
gr_y[i,] = dat_kde_BT$y
}
head(gr_y)
gr_y_var = diag(var(gr_y))
gr_y_var
gr_y_sd = sqrt(gr_y_var)
gr_y_sd
plot(x=dat_kde$x,y=gr_y_sd, type="l", lwd=3)
plot(dat_kde, lwd=3, col="dodgerblue")
lines(x=dat_kde$x,y=gr_y_sd, lwd=3, col="brown")
legend("topleft",c("KDE","sd of KDE"), lwd=6, col=c("dodgerblue","brown"))
## CI of the KDE -- dashed line
plot(dat_kde, lwd=3, col="blue", ylim=c(0,0.04),
main="90% CI of the KDE using bootstrap SD")
lines(x=dat_kde$x,y=dat_kde$y+qnorm(0.95)*gr_y_sd, lwd=3, col="dodgerblue",
lty=2)
lines(x=dat_kde$x,y=dat_kde$y-qnorm(0.95)*gr_y_sd, lwd=3, col="dodgerblue",
lty=2)
## CI of the KDE -- "band"
plot(dat_kde, lwd=3, col="blue", ylim=c(0,0.04),
main="90% CI of the KDE using bootstrap SD")
polygon(x=c(dat_kde$x, rev(dat_kde$x)),
y=c(dat_kde$y+qnorm(0.95)*gr_y_sd, rev(dat_kde$y-qnorm(0.95)*gr_y_sd)),
col='skyblue', border = 'skyblue')
lines(dat_kde,  lwd=3, col="blue")
## quantile approach and the use of sapply
q95_loop = rep(NA,1000)
q05_loop = rep(NA,1000)
for(i in 1:1000){
q95_loop[i] = quantile(gr_y[,i], 0.95)
q05_loop[i] = quantile(gr_y[,i], 0.05)
}
## sapply: a faster way to do a loop
help(sapply)
sapply(1:10, exp)
sapply(1:10, function(x){dnorm(1,mean=0,sd=x)})
q95 = sapply(1:1000, function(x){quantile(gr_y[,x], 0.95)})
# apply to each grid point
q05 = sapply(1:1000, function(x){quantile(gr_y[,x], 0.05)})
# CI of the KDE -- dashed line
plot(dat_kde, lwd=3, col="purple", ylim=c(0,0.04),
main="90% CI of the KDE using bootstrap SD")
lines(x=dat_kde$x,y=q95, lwd=3, col="violet",
lty=2)
lines(x=dat_kde$x,y=q05, lwd=3, col="violet",
lty=2)
# CI of the KDE -- "band"
plot(dat_kde, lwd=3, col="purple", ylim=c(0,0.04),
main="90% CI of the KDE using bootstrap SD")
polygon(x=c(dat_kde$x, rev(dat_kde$x)),
y=c(q95, rev(q05)),
col='violet', border = 'violet')
lines(dat_kde,  lwd=3, col="purple")
###                          ###
### Part 3: KDE--Uncertainty ###
###                          ###
### analyze the bias and variance
n = 1000
?polygon
###                          ###
### Part 3: KDE--Uncertainty ###
###                          ###
### analyze the bias and variance
n = 1000
h0 = 0.2
rmultinom(1, 100, prob=c(0.3,0.3,0.4))
?rmultinom
rmultinom(1, 100, prob=c(0.3,0.3,0.4))
U = rmultinom(1, n, prob=c(0.3,0.3,0.4))
U
mu1 = 0
mu2 = 4
mu3 = 8
sd1 = sd2 = 1
sd3 = 1
dat1 = c(rnorm(U[1], mean=mu1,sd=sd1), rnorm(U[2], mean=mu2,sd=sd2),
rnorm(U[3],mean=mu3,sd=sd3))
dat1
hist(dat1, breaks=30, probability = T, col="palegreen")
hist(dat1, breaks=30, probability = T, col="palegreen")
### KDE
dat1_kde = density(dat1, from=-5, to=15, n=1000,bw=h0)
true_density = 0.3*dnorm(dat1_kde$x,mean=mu1,sd=sd1)+
0.3*dnorm(dat1_kde$x,mean=mu2,sd=sd2)+
0.4*dnorm(dat1_kde$x,mean=mu3,sd=sd3)
plot(x=dat1_kde$x, y=true_density, ylim=c(0,0.35), type="l",
lwd=3, col="blue")
lines(dat1_kde, lwd=3, col="red")
legend("topleft",c("KDE","True"),lwd=6, col=c("blue","red"))
### a realization of difference
diff = dat1_kde$y- true_density
plot(x=dat1_kde$x,diff, type="l", lwd=3, col="purple",
ylim=c(-0.05,0.05))
abline(h=0, lwd=3, col="gray")
### Monte Carlo Estimation of the errors
N = 10000
dat1_kdey_MC = matrix(NA, nrow=N, ncol=1000)
for(i in 1:N){
U = rmultinom(1, n, prob=c(0.30,0.30,0.4))
dat1_MC = c(rnorm(U[1], mean=mu1,sd=sd1), rnorm(U[2], mean=mu2,sd=sd2),
rnorm(U[3], mean=mu3,sd=sd3))
dat1_kde_MC = density(dat1_MC, from=-5, to=15, n=1000,bw =h0)
dat1_kdey_MC[i,] = dat1_kde_MC$y
}
dat1_kdey_var = sapply(1:1000, function(x){var(dat1_kdey_MC[,x])})
dat1_kdey_mse = colSums((t(t(dat1_kdey_MC)- true_density))^2)/N
plot(x=dat1_kde$x, y=true_density, ylim=c(0,0.35), type="l",
lwd=3, col="blue")
lines(x=dat1_kde$x, y=sqrt(dat1_kdey_var), col="red")
lines(x=dat1_kde$x, y=sqrt(dat1_kdey_mse), col="black")
sum(dat1_kdey_var)
# ~ integrated variance (without scaling)
sum(dat1_kdey_mse)
## scaling:
sum(dat1_kdey_var)*(dat1_kde$x[2]-dat1_kde$x[1])
sum(dat1_kdey_mse)*(dat1_kde$x[2]-dat1_kde$x[1])
### What happen if we choose different h?
### Now we perform a "simulation study" to understand it.
### This is a very common approach in practice to analyze the performance
### of a certain estimator.
h0_seq = seq(from=0.1,to=0.5, by=0.05)
# consider several possible smoothing bandwidth
N = 10000
var_seq = rep(NA, length(h0_seq))
mise_seq = rep(NA, length(h0_seq))
for(i_MC in 1:length(h0_seq)){
h0 = h0_seq[i_MC]
# choose the smoothing parameter
dat1_kdey_MC = matrix(NA, nrow=N, ncol=1000)
for(i in 1:N){
U = rmultinom(1, n, prob=c(0.3,0.3,0.4))
dat1_MC = c(rnorm(U[1], mean=mu1,sd=sd1), rnorm(U[2], mean=mu2,sd=sd2),
rnorm(U[3], mean=mu3,sd=sd3))
dat1_kde_MC = density(dat1_MC, from=-5, to=15, n=1000,bw =h0)
dat1_kdey_MC[i,] = dat1_kde_MC$y
}
dat1_kdey_var = sapply(1:1000, function(x){var(dat1_kdey_MC[,x])})
dat1_kdey_mse = colSums((t(t(dat1_kdey_MC)- true_density))^2)/N
var_seq[i_MC] = sum(dat1_kdey_var)
mise_seq[i_MC] = sum(dat1_kdey_mse)
print(i_MC)
# print out each iteration
}
plot(x= h0_seq, y= mise_seq, type="l", ylim=c(0,0.2),lwd=3,
ylab="errors", xlab="smoothing bandwidth", col="brown")
lines(x= h0_seq, y= var_seq, lwd=3, col="limegreen")
legend("bottomleft", c("MISE","Integrated VAR"), lwd=6,
col=c("brown","limegreen"))
###                ###
### Part 4: 2D KDE ###
###                ###
library(KernSmooth)
data1 = cbind(iris$Sepal.Length, iris$Petal.Length)
plot(data1)
iris_kde <- bkde2D(data1, bandwidth = 0.25,
gridsize = c(51,51),
range.x=list(c(4,8),c(1,7)))
contour(x=iris_kde$x1,y=iris_kde$x2,
z=iris_kde$fhat, lwd=3,
main="Density Contour (Iris Data)",
xlab="Sepal.Length", ylab="Petal.Length")
contour(x=iris_kde$x1,y=iris_kde$x2,
z=iris_kde$fhat, lwd=3,
main="Density Contour (Iris Data)",
xlab="Sepal.Length", ylab="Petal.Length",
nlevels=20, col=c("blue"))
points(data1, col="red",pch=20)
### perspective plot
persp(x=iris_kde$x1,y=iris_kde$x2,
z=iris_kde$fhat, col="palegreen",
xlab="Sepal.Length", ylab="Petal.Length",
zlab="Density", theta=150, phi=30)
?persp
for(w in (1:36)*10){
persp(x=iris_kde$x1,y=iris_kde$x2,
z=iris_kde$fhat, col="palegreen",
xlab="Sepal.Length", ylab="Petal.Length",
zlab="Density", theta=w, phi=40)
Sys.sleep(0.1)
}
## a non-stopping animation
x0 = 1
w = 10
while(x0<2){
persp(x=iris_kde$x1,y=iris_kde$x2,
z=iris_kde$fhat, col="palegreen",
xlab="Sepal.Length", ylab="Petal.Length",
zlab="Density", theta=w, phi=40)
w= w+5
Sys.sleep(0.1)
}
for(w in c((0:9)*10, (9:0)*10)){
persp(x=iris_kde$x1,y=iris_kde$x2,
z=iris_kde$fhat, col="skyblue",
xlab="Sepal.Length", ylab="Petal.Length",
zlab="Density", theta=10, phi=w)
Sys.sleep(1)
}
### using the image function
image(x=iris_kde$x1,y=iris_kde$x2,
z=iris_kde$fhat, xlab="Sepal.Length",
ylab="Petal.Length", main="Density (Heat Map)")
points(data1, pch=20)
## change the color using Palette
colP = colorRampPalette(c("white","dodgerblue","purple"))
image(x=iris_kde$x1,y=iris_kde$x2,
z=iris_kde$fhat, xlab="Sepal.Length",
ylab="Petal.Length", main="Density (Heat Map)",
col=colP(50))
points(data1, pch=20, cex=0.5)
### colored contour
col_tmp <- colorRampPalette(c("white","limegreen","orchid"))(10)
level_tmp <- (0:10)/10*max(c(iris_kde$fhat))
# defining the levels and the corresponding colors
plot(NULL, xlim=c(4,8), ylim=c(1,7),
main="Density Contour (Iris Data)",
xlab="Sepal.Length", ylab="Petal.Length")
# this makes an empty plot
.filled.contour(x=iris_kde$x1,y=iris_kde$x2,
z=iris_kde$fhat,
levels=level_tmp,
col=col_tmp)
# this filled in the colored contours
points(data1, col="black",pch=20)
#### Exercise 4:
#### In this problem, we will analyze the 'faithful' dataset using the
#### current eruption time versus the next eruption time:
data_new = cbind(faithful[1:271,1],faithful[2:272,1])
plot(data_new)
reg_wild_bt <- matrix(NA, nrow=B, ncol=4)
for (ii in 1:B) {
sp_y <- rnorm(n, petal_wdt_ln$residuals) * rnorm(n) + y_predict
sp_dt <- data.frame(Sepal.Length=iris$Sepal.Length, Sepal.Width=iris$Sepal.Width,
Petal.Length=iris$Petal.Length, Petal.Width=sp_y)
sp_ln <- lm(Petal.Width~Sepal.Length + Sepal.Width + Petal.Length, data=sp_dt)
reg_wild_bt[ii,] <- sp_ln$coefficients
}
# question 1-a
petal_wdt_ln <- lm(Petal.Width~Sepal.Length + Sepal.Width + Petal.Length, data=iris)
for (ii in 1:B) {
sp_y <- rnorm(n, petal_wdt_ln$residuals) * rnorm(n) + y_predict
sp_dt <- data.frame(Sepal.Length=iris$Sepal.Length, Sepal.Width=iris$Sepal.Width,
Petal.Length=iris$Petal.Length, Petal.Width=sp_y)
sp_ln <- lm(Petal.Width~Sepal.Length + Sepal.Width + Petal.Length, data=sp_dt)
reg_wild_bt[ii,] <- sp_ln$coefficients
}
# residual bootstrap
y_predict <- predict(petal_wdt_ln)
reg_resid_bt <- matrix(NA, nrow=B, ncol=4)
for (ii in 1:B) {
sp_index <- sample(n, n, replace=T)
sp_y <- petal_wdt_ln$residuals[sp_index] + y_predict
sp_dt <- data.frame(Sepal.Length=iris$Sepal.Length, Sepal.Width=iris$Sepal.Width,
Petal.Length=iris$Petal.Length, Petal.Width=sp_y)
sp_ln <- lm(Petal.Width~Sepal.Length + Sepal.Width + Petal.Length, data=sp_dt)
reg_resid_bt[ii,] <- sp_ln$coefficients
}
var_resid_bt <- cal_var(reg_resid_bt, 4)
petal_wdt_ln$coefficients
# quearion 1-b
# empirical bootstrap
cal_var <- function(dt, col_num) {
result <- rep(NA, col_num)
for (ii in 1:col_num) {
result[ii] <- var(dt[, ii])
}
return(result)
}
n <- nrow(iris)
B <- 10000
reg_ep_bt <- matrix(NA, nrow=B, ncol=4)
for (ii in 1:B) {
sp_index <- sample(n, n, replace=T)
sp_dt <- iris[sp_index,]
sp_ln <- lm(Petal.Width~Sepal.Length + Sepal.Width + Petal.Length, data=sp_dt)
reg_ep_bt[ii,] <- sp_ln$coefficients
}
var_ep_bt <- cal_var(reg_ep_bt, 4)
# residual bootstrap
y_predict <- predict(petal_wdt_ln)
reg_resid_bt <- matrix(NA, nrow=B, ncol=4)
for (ii in 1:B) {
sp_index <- sample(n, n, replace=T)
sp_y <- petal_wdt_ln$residuals[sp_index] + y_predict
sp_dt <- data.frame(Sepal.Length=iris$Sepal.Length, Sepal.Width=iris$Sepal.Width,
Petal.Length=iris$Petal.Length, Petal.Width=sp_y)
sp_ln <- lm(Petal.Width~Sepal.Length + Sepal.Width + Petal.Length, data=sp_dt)
reg_resid_bt[ii,] <- sp_ln$coefficients
}
var_resid_bt <- cal_var(reg_resid_bt, 4)
# wild bootstrap
reg_wild_bt <- matrix(NA, nrow=B, ncol=4)
for (ii in 1:B) {
sp_y <- rnorm(n, petal_wdt_ln$residuals) * rnorm(n) + y_predict
sp_dt <- data.frame(Sepal.Length=iris$Sepal.Length, Sepal.Width=iris$Sepal.Width,
Petal.Length=iris$Petal.Length, Petal.Width=sp_y)
sp_ln <- lm(Petal.Width~Sepal.Length + Sepal.Width + Petal.Length, data=sp_dt)
reg_wild_bt[ii,] <- sp_ln$coefficients
}
var_wild_bt <- cal_var(reg_wild_bt, 4)
var_result <- rbind(var_ep_bt, var_resid_bt, var_wild_bt)
rownames(var_result) <- c('Empirical bt', 'Residual bt', 'Wild bt')
colnames(var_result) <- c('Intercept', 'Sepal.Length', 'Sepal.Width', 'Petal.Length')
var_result
reg_wild_bt <- matrix(NA, nrow=B, ncol=4)
for (ii in 1:B) {
sp_y <- rnorm(n, sd=abs(petal_wdt_ln$residuals)) + y_predict
sp_dt <- data.frame(Sepal.Length=iris$Sepal.Length, Sepal.Width=iris$Sepal.Width,
Petal.Length=iris$Petal.Length, Petal.Width=sp_y)
sp_ln <- lm(Petal.Width~Sepal.Length + Sepal.Width + Petal.Length, data=sp_dt)
reg_wild_bt[ii,] <- sp_ln$coefficients
}
var_wild_bt <- cal_var(reg_wild_bt, 4)
var_result <- rbind(var_ep_bt, var_resid_bt, var_wild_bt)
rownames(var_result) <- c('Empirical bt', 'Residual bt', 'Wild bt')
colnames(var_result) <- c('Intercept', 'Sepal.Length', 'Sepal.Width', 'Petal.Length')
var_result
smoke_resid <- (smoke_dt - smoke_mean) / smoke_sd
smoke_sd <- sd(smoke_dt)
smoke_mean <- mean(smoke_dt)
baby_dt <- read.table('babiesI.data', header=T)
smoke_dt <- baby_dt[(baby_dt$smoke==1),]
nonsmoke_dt <- baby_dt[(baby_dt$smoke==0),]
smoke_sd <- sd(smoke_dt)
smoke_sd
smoke_mean <- mean(smoke_dt)
somke_mean
smoke_mean
smoke_dt <- baby_dt[(baby_dt$smoke==1),]
head(smoke_dt)
smoke_sd <- sd(smoke_dt$bwt)
smoke_mean <- mean(smoke_dt$bwt)
smoke_resid <- (smoke_dt - smoke_mean) / smoke_sd
head(smoke_resid)
smoke_resid <- (smoke_dt$bwt - smoke_mean) / smoke_sd
head(smoke_resid)
n <- nrow(smoke_dt)
B = 10000
n <- nrow(smoke_dt)
B = 10000
D_values <- rep(NA, B)
for (ii in 1:B) {
sim_z <- rnorm(n=10)
sim_resid <- sqrt(n-1) * (sim_z - mean(sim_z)) /
sqrt(sum((sim_z - mean(sim_z))^2))
sim_D <- (n-1)^(-3/2) * sum(sim_resid^3)
D_values[ii] <- sim_D
}
hist(D_values, probability=T, xlim=c(-1, 1),
xlab='D-skew values', main='Histogram of D-skew')
D_values <- rep(NA, B)
for (ii in 1:B) {
sim_z <- rnorm(n)
sim_resid <- sqrt(n-1) * (sim_z - mean(sim_z)) /
sqrt(sum((sim_z - mean(sim_z))^2))
sim_D <- (n-1)^(-3/2) * sum(sim_resid^3)
D_values[ii] <- sim_D
}
hist(D_values, probability=T, xlim=c(-1, 1),
xlab='D-skew values', main='Histogram of D-skew')
hist(D_values, probability=T,
xlab='D-skew values', main='Histogram of D-skew')
smoke_D <- (n-1)^(-3/2) * sum(smoke_resid^3)
smoke_D
abline(h=smoke_D
abline(v=smoke_D)
abline(v=smoke_D)
abline(v=smoke_D, lwd=3, lty=3)
hist(D_values, probability=T, xlab='D-skew values', main='Histogram of D-skew')
abline(v=smoke_D, lwd=3, lty=3)
n
p_value <- length(D_values[D_values < smoke_D | D_values > abs(smoke_D)]) / B
p_value
n <- nrow(nonsmoke_dt)
n
#nonsmoke
nonsmoke_sd <- sd(nonsmoke_dt$bwt)
nonsmoke_mean <- mean(nonsmoke_dt$bwt)
nonsmoke_resid <- (nonsmoke_dt$bwt - nonsmoke_mean) / nonsmoke_sd
nonsmoke_D <- (n-1)^(-3/2) * sum(nonsmoke_resid^3)
n <- nrow(nonsmoke_dt)
B = 10000
D_values <- rep(NA, B)
for (ii in 1:B) {
sim_z <- rnorm(n)
sim_resid <- sqrt(n-1) * (sim_z - mean(sim_z)) /
sqrt(sum((sim_z - mean(sim_z))^2))
sim_D <- (n-1)^(-3/2) * sum(sim_resid^3)
D_values[ii] <- sim_D
}
hist(D_values, probability=T, xlab='D-skew values', main='Histogram of D-skew')
abline(v=nonsmoke_D, lwd=3, lty=3)
nonsmoke_D
p_value <- length(D_values[D_values < nonsmoke_D | D_values > abs(nonsmoke_D)]) / B
p-value
p_value
library(ggplot2)
setwd('/Users/nantang/Google Drive/STAT 403/Project')
car_dt <- read.csv('true_car_listings.csv', header=T)
sample_index <- sample(nrow(car_dt), 1000)
car_sample <- car_dt[sample_index, ]
nrow(car_dt[which(car_dt$Make=='Ferrari'),])
ferrari_dt <- car_dt[which(car_dt$Make=='Ferrari'),]
p1 <- ggplot(data = ferrari_dt, aes(x=Mileage, y=Price)) +
geom_point(aes(col=Year)) +
geom_smooth(method=loess, se=F)
p2 <- ggplot(data = ferrari_dt, aes(x=State, y=Price)) +
geom_point(aes(col=Model)) +
geom_smooth(method=lm, se=F)
plot(p1)
plot(p2)
head(car_dt)
levels(as.factor(car_dt$City))
head(car_dt)
levels(as.factor(car_dt$Vn))
levels(as.factor(car_dt$Vin))
head(car_dt)
plot(nonsmoke_resid, pch=16, cex=0.5, ylab='Standardized Residuals', ylim=c(-5, 5),
main='Non-Smoker\'s Baby Weight Residual Plot')
plot(nonsmoke_resid, pch=16, cex=0.5, ylab='Standardized Residuals', ylim=c(-4, 4),
main='Non-Smoker\'s Baby Weight Residual Plot')
abline(h=0, lty=1, lwd=1, col='gray')
abline(h=c(0, -3, 3), lty=1, lwd=1, col='gray')
rep(0.2, 5)
# question 5-b
chisq.test(p_value_dt, p=rep(0.2, 5))
# question 5-b
hist(p_value_dt, breaks=5)
interval_count <- c(10, 4, 5, 5, 3)
sum(interval_count)
length(p-value)
length(p_value_dt)
chisq.test(interval_count, p=rep(0.2, 5))
abline(v=smoke_D, lwd=3, lty=3)
hist(D_values, probability=T, xlab='D-skew values', main='Histogram of D-skew (Smoker)')
abline(v=smoke_D, lwd=3, lty=3)
hist(D_values, probability=T, xlab='D-skew values', main='Histogram of D-skew (Non-Smoker)')
abline(v=nonsmoke_D, lwd=3, lty=3)
est_prob
# question 2-3
est_prob <- predict(binom_logistic, type='response',
data.frame(gpa=3.7, gre=500), se.fit=T)
# question 2-a
setwd('/Users/nantang/Google Drive/STAT 403/HW/hw7')
admission_dt <- read.csv('binary.csv', header =T)
binom_logistic <- glm(admit~gpa + gre, data=admission_dt, family='binomial')
B <- 10000
n <- nrow(admission_dt)
# question 2-3
est_prob <- predict(binom_logistic, type='response',
data.frame(gpa=3.7, gre=500), se.fit=T)
est_prob
# question 2-3
est_prob <- predict(binom_logistic, type='response',
data.frame(gpa=3.7, gre=500))
predict(binom_logistic, type='response',
+                     data.frame(gpa=3.7, gre=500))
predict(binom_logistic, type='response',
+                     data.frame(gpa=3.7, gre=500))
predict(binom_logistic, type='response',
+                     data.frame(gpa=3.7, gre=500))
prob_bt <- rep(NA, B)
for (ii in 1:B) {
sp_index <- sample(n, n, replace=T)
sp_dt <- admission_dt[sp_index,]
sp_logis <- glm(admit~gpa + gre, data=sp_dt, family='binomial')
prob_bt[ii] <- predict(sp_logis, type='response', data.frame(gpa=3.7, gre=500))
}
lower_bd <- quantile(prob_bt, 0.05)
upper_bd <- quantile(prob_bt, 0.95)
lower
lower_bd
upper_bd
hist(prob_bt)
probs_bt <- matrix(NA, nrow=B, ncol=2)
for (ii in 1:B) {
sp_index <- sample(n, n, replace=T)
sp_dt <- admission_dt[sp_index,]
sp_logis <- glm(admit~gpa + gre, data=sp_dt, family='binomial')
probs_bt[ii, 1] <- predict(sp_logis, type='response', data.frame(gpa=2.3, gre=700))
probs_bt[ii, 2] <- predict(sp_logis, type='response', data.frame(gpa=3.9, gre=670))
}
prob1_sd <- sd(probs_bt[,1])
prob2_sd <- sd(probs_bt[,2])
prob1_mean <- mean(probs_bt[,1])
prob2_mean <- mean(probs_bt[,2])
prob1_mean
prob2_mean
t.test(prob_bt[,1], prob_bt[,2])
t.test(probs_bt[,1], probs_bt[,2])
z_score <- (prob1_mean - prob2_mean) / sqrt(prob1_sd^2 + prob2_sd^2)
p_value <- 2 * pnorm(z_score)
p_value
delta <- probs_bt[,1] - probs_bt[,2]
delta_sd <- sd(delta)
observed_prob1 <- predict(binom_logistic, type='response', data.frame(gpa=2.3, gre=700))
observed_delta <- observed_prob1 - observed_prob2
observed_prob2 <- predict(binom_logistic, type='response', data.frame(gpa=3.9, gre=670))
observed_delta <- observed_prob1 - observed_prob2
p-value <- delta[delta < observed_delta | delta > abs(observed_delta)]
p_value <- delta[delta < observed_delta | delta > abs(observed_delta)]
hist(delta)
hist(delta)
delta_sd
pnorm(observed_delta, sd=delta_sd)
observed_delta
p_value <- 2 * pnorm(observed_delta, sd=delta_sd)
p_value
